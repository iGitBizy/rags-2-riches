{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install pytorch\n",
    "!pip install FLAX\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model pipeline\n",
    "summarizer = pipeline(task='summarization', model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Pass the long text to the model\n",
    "output = summarizer(long_text, max_length=50, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Access and print the summarized text\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using PreTrained Models\n",
    "\n",
    "#Generating Text\n",
    "generator = pipeline(task='text-generation', model=\"distilgpt2\")\n",
    "\n",
    "prompt = \"The Gion neighborhood in Kyoto is famous for\"\n",
    "\n",
    "output = generator(prompt, max_lenght=150, pad_token_id = generator.tokenization.eos_token_id)\n",
    "\n",
    "print(output[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text = \"Este curso sobre LLMs se estÃ¡ poniendo muy interesante\"\n",
    "\n",
    "# Define the pipeline\n",
    "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the Spanish text\n",
    "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(translations[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Transformers\n",
    "Types \n",
    "\tEncoder only:\n",
    "\t\t- understand input text\n",
    "\t\t- no sequential output\n",
    "\t\t- common tasks:\n",
    "\t\t\t- Text classification\n",
    "\t\t\t- Sentiment Analysis\n",
    "\t\t\t- Extractive Question answering (where output is extract of text or label) (Bert \n",
    "\n",
    "\t#checkout model architecture\n",
    "\tllm = pipeline(modeld=\"bert-base-uncase\")\n",
    "\tprint(llm.model)\n",
    "\tprint(llm.model.config)\n",
    "\tprint(llm.model.config.is_decoder)\n",
    "\tprint(llm.model.config.is_encoder_decoder)\n",
    "\n",
    "\tDecoder only:\n",
    "\t\t- only focus on output\n",
    "\t\t- common task:\n",
    "\t\t\t- Text Generation\n",
    "\t\t\t- Generative Q&A (sentences or paragraphs) (chaptgpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tEncoder-decoder:\n",
    "\t\t- Understand and process the input and output\n",
    "\t\t- common task:\n",
    "\t\t\t- Translation\n",
    "\t\t\t- Summarization (T5, BART models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "output = qa(question=question, context=text)\n",
    "print(output['answer'])\n",
    "\n",
    "\n",
    "#Second example\n",
    "question = \"Who painted the Mona Lisa?\"\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
    "\n",
    "#Define Context \n",
    "text = \"\\nThe Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the most known, visited, talked about, and sung about work of art in the world. The painting's novel qualities include the subject's enigmatic expression, the monumentality of the composition, and the subtle modeling of forms.\\n\"\n",
    "\n",
    "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning LLMs\n",
    "Pipeline = streamlines \n",
    "autotclasses - more custom , fine tuning\n",
    "from transforms impor AutoModel, AutoTokenizer, \n",
    "\n",
    "LLM dev cycle\n",
    "Pretrain > Pretrained FM > Fine tuning > Tine tuned model \n",
    "\n",
    "FINE TUNE MODEL \n",
    "LEVERAGE A PRETRAINED MODEL FROM HUGGING FACE AND FINE TUNE IT SPECIFIC DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading a dataset for fine-tuning\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load_dataset() loads a dataset from hugging face hub\n",
    "# use .shard() to split dataset into chunks and selecting\n",
    "# first chunk as index 0, we do this speed up training\n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")\n",
    "train_data = data.shard(num_shards=4, index=0)p\n",
    "test_data = load_dataset(\"imdb\", split=\"test\")\n",
    "test_data = data.shard(num_shards=4, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Auto Classes\n",
    "from transformers import AutoModel, Autotokenizer,\n",
    "from transformers import AutoModelForSequenceClassification #task specific automodel\n",
    "\n",
    "# Loads a specified pretrained model with larned weights \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-based-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize Data, enabling padding, truncation, max_length for efficiency\n",
    "# set the return_tensors to pt to return PyTorch tensors since the model expects this format\n",
    "tokenized_training_data = tokenizer(train_data[\"text\", return_tensors=\"pt\", padding=True,\n",
    "truncation=True, max_length=64)\n",
    "\n",
    "tokenized_test_data = tokenizer(test_data[\"text\"], return_tensors=\"pt\", padding=True, \n",
    "truncation=True, max_length=)\n",
    "\n",
    "print(tokenized_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Row by Row\n",
    "def tokenzize_function(text_data):\n",
    "    return tokenizer(text_data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,\n",
    "\t\t\tmax_length=64)\n",
    "# Tokenize in batches\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "\n",
    "MAPPING TOKENIZATION\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"interaction\"], \n",
    "                     return_tensors='pt', \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "print(tokenized_by_row)\n",
    "\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_training_data = tokenizer(train_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "tokenized_test_data = tokenizer(test_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "print(tokenized_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Through TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an instance of TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./finetuned\",\n",
    "\n",
    "  # Set the evaluation strategy\n",
    "  evaluation_strategy=\"epoch\",\n",
    "\n",
    "  # Specify the number of epochs\n",
    "  num_train_epochs=3,\n",
    "  learning_rate=2e-5,\n",
    "\n",
    "  # Set the batch sizes\n",
    "  per_device_train_batch_size=3,\n",
    "  per_device_eval_batch_size=3,\n",
    "  weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH TRAINING ARGUMENTS IN PLACE USE TRAINER CLASS TO TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # Assign the training arguments and tokenizer\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH TRAINING ARGUMENTS IN PLACE USE TRAINER CLASS TO TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "USING FINE-TUNED MODEL\n",
    "input_text = [\"I'd just like to say, I love the product! Thank you!\"]\n",
    "\n",
    "# Tokenize the new data\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pass the tokenized inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the new predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "label_map = {0: \"Low risk\", 1: \"High risk\"}\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    churn_label = label_map[predicted_label]\n",
    "    print(f\"\\n Input Text {i + 1}: {input_text[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with OneShot \n",
    "## Model trained using only one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include an example in the input ext\n",
    "input_text = \"\"\"\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Classify the sentiment of this sentence as either positive or negative.\n",
    "Example:\n",
    "Text: \"The food was delicious\"\n",
    "Sentiment: Positive\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Apply the example to the model\n",
    "result = model(input_text, max_length=100)\n",
    "\n",
    "print(result[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
