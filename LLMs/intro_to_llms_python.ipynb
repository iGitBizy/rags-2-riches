{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jessi\\ml_path\\rags_2_riches\\rags-2-riches\\llms\\.venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "Using cached huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.1 tokenizers-0.21.1 transformers-4.50.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install torch\n",
    "# %pip install FLAX\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bffe13c0cca4cc5907f558aa814ff83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jessi\\.cache\\huggingface\\hub\\models--cnicu--t5-small-booksum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1976\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1206\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1149\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf2xla\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxla\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic_slice\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     TFBaseModelOutput,\n\u001b[32m     33\u001b[39m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     34\u001b[39m     TFSeq2SeqLMOutput,\n\u001b[32m     35\u001b[39m     TFSeq2SeqModelOutput,\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m long_text = \u001b[33m\"\u001b[39m\u001b[33mBUDAPEST, Hungary: Hungary will start the process to withdraw from the International Criminal Court, an official said Thursday, just as Israeli Prime Minister Benjamin Netanyahu arrived to red carpet treatment in the country\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms capital despite an arrest warrant from the worlds only permanent global tribunal for war crimes and genocide.Prime Minister Viktor Orbán gave the Israeli leader a welcome with full military honors in Budapest\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Castle District.The two close allies stood side by side as a military band played and an elaborate procession of soldiers on horseback and carrying swords and bayoneted rifles marched by.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load the model pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m summarizer = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcnicu/t5-small-booksum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Pass the long text to the model\u001b[39;00m\n\u001b[32m      9\u001b[39m output = summarizer(long_text, max_length=\u001b[32m50\u001b[39m, clean_up_tokenization_spaces=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:266\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m         classes.append(_class)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     _class = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    268\u001b[39m         classes.append(_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1965\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   1964\u001b[39m     module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   1967\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1964\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     value = Placeholder\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jessi\\ML_PATH\\RAGs_2_Riches\\rags-2-riches\\LLMs\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1978\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1979\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1981\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "long_text = \"BUDAPEST, Hungary: Hungary will start the process to withdraw from the International Criminal Court, an official said Thursday, just as Israeli Prime Minister Benjamin Netanyahu arrived to red carpet treatment in the country's capital despite an arrest warrant from the worlds only permanent global tribunal for war crimes and genocide.Prime Minister Viktor Orbán gave the Israeli leader a welcome with full military honors in Budapest's Castle District.The two close allies stood side by side as a military band played and an elaborate procession of soldiers on horseback and carrying swords and bayoneted rifles marched by.\"\n",
    "\n",
    "# Load the model pipeline\n",
    "summarizer = pipeline(task='summarization', model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Pass the long text to the model\n",
    "output = summarizer(long_text, max_length=50, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Access and print the summarized text\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using PreTrained Models\n",
    "\n",
    "#Generating Text\n",
    "generator = pipeline(task='text-generation', model=\"distilgpt2\")\n",
    "\n",
    "prompt = \"The Gion neighborhood in Kyoto is famous for\"\n",
    "\n",
    "output = generator(prompt, max_lenght=150, pad_token_id = generator.tokenization.eos_token_id)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text = \"Este curso sobre LLMs se está poniendo muy interesante\"\n",
    "\n",
    "# Define the pipeline\n",
    "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the Spanish text\n",
    "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(translations[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Transformers\n",
    "Types \n",
    "\tEncoder only:\n",
    "\t\t- understand input text\n",
    "\t\t- no sequential output\n",
    "\t\t- common tasks:\n",
    "\t\t\t- Text classification\n",
    "\t\t\t- Sentiment Analysis\n",
    "\t\t\t- Extractive Question answering (where output is extract of text or label) (Bert \n",
    "\n",
    "\t#checkout model architecture\n",
    "\tllm = pipeline(modeld=\"bert-base-uncase\")\n",
    "\tprint(llm.model)\n",
    "\tprint(llm.model.config)\n",
    "\tprint(llm.model.config.is_decoder)\n",
    "\tprint(llm.model.config.is_encoder_decoder)\n",
    "\n",
    "\tDecoder only:\n",
    "\t\t- only focus on output\n",
    "\t\t- common task:\n",
    "\t\t\t- Text Generation\n",
    "\t\t\t- Generative Q&A (sentences or paragraphs) (chaptgpt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tEncoder-decoder:\n",
    "\t\t- Understand and process the input and output\n",
    "\t\t- common task:\n",
    "\t\t\t- Translation\n",
    "\t\t\t- Summarization (T5, BART models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "output = qa(question=question, context=text)\n",
    "print(output['answer'])\n",
    "\n",
    "\n",
    "#Second example\n",
    "question = \"Who painted the Mona Lisa?\"\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
    "\n",
    "#Define Context \n",
    "text = \"\\nThe Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the most known, visited, talked about, and sung about work of art in the world. The painting's novel qualities include the subject's enigmatic expression, the monumentality of the composition, and the subtle modeling of forms.\\n\"\n",
    "\n",
    "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning LLMs\n",
    "Pipeline = streamlines \n",
    "autotclasses - more custom , fine tuning\n",
    "from transforms impor AutoModel, AutoTokenizer, \n",
    "\n",
    "LLM dev cycle\n",
    "Pretrain > Pretrained FM > Fine tuning > Tine tuned model \n",
    "\n",
    "FINE TUNE MODEL \n",
    "LEVERAGE A PRETRAINED MODEL FROM HUGGING FACE AND FINE TUNE IT SPECIFIC DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading a dataset for fine-tuning\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load_dataset() loads a dataset from hugging face hub\n",
    "# use .shard() to split dataset into chunks and selecting\n",
    "# first chunk as index 0, we do this speed up training\n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")\n",
    "train_data = data.shard(num_shards=4, index=0)p\n",
    "test_data = load_dataset(\"imdb\", split=\"test\")\n",
    "test_data = data.shard(num_shards=4, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Auto Classes\n",
    "from transformers import AutoModel, Autotokenizer,\n",
    "from transformers import AutoModelForSequenceClassification #task specific automodel\n",
    "\n",
    "# Loads a specified pretrained model with larned weights \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-based-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize Data, enabling padding, truncation, max_length for efficiency\n",
    "# set the return_tensors to pt to return PyTorch tensors since the model expects this format\n",
    "tokenized_training_data = tokenizer(train_data[\"text\", return_tensors=\"pt\", padding=True,\n",
    "truncation=True, max_length=64)\n",
    "\n",
    "tokenized_test_data = tokenizer(test_data[\"text\"], return_tensors=\"pt\", padding=True, \n",
    "truncation=True, max_length=)\n",
    "\n",
    "print(tokenized_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Row by Row\n",
    "def tokenzize_function(text_data):\n",
    "    return tokenizer(text_data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,\n",
    "\t\t\tmax_length=64)\n",
    "# Tokenize in batches\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "\n",
    "MAPPING TOKENIZATION\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"interaction\"], \n",
    "                     return_tensors='pt', \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     max_length=64)\n",
    "\n",
    "# Tokenize row by row\n",
    "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
    "print(tokenized_by_row)\n",
    "\n",
    "tokenized_in_batches = train_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_training_data = tokenizer(train_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "tokenized_test_data = tokenizer(test_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "print(tokenized_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Through TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an instance of TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./finetuned\",\n",
    "\n",
    "  # Set the evaluation strategy\n",
    "  evaluation_strategy=\"epoch\",\n",
    "\n",
    "  # Specify the number of epochs\n",
    "  num_train_epochs=3,\n",
    "  learning_rate=2e-5,\n",
    "\n",
    "  # Set the batch sizes\n",
    "  per_device_train_batch_size=3,\n",
    "  per_device_eval_batch_size=3,\n",
    "  weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH TRAINING ARGUMENTS IN PLACE USE TRAINER CLASS TO TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # Assign the training arguments and tokenizer\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_test_data,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH TRAINING ARGUMENTS IN PLACE USE TRAINER CLASS TO TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "USING FINE-TUNED MODEL\n",
    "input_text = [\"I'd just like to say, I love the product! Thank you!\"]\n",
    "\n",
    "# Tokenize the new data\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pass the tokenized inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the new predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "label_map = {0: \"Low risk\", 1: \"High risk\"}\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    churn_label = label_map[predicted_label]\n",
    "    print(f\"\\n Input Text {i + 1}: {input_text[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with OneShot \n",
    "## Model trained using only one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include an example in the input ext\n",
    "input_text = \"\"\"\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Classify the sentiment of this sentence as either positive or negative.\n",
    "Example:\n",
    "Text: \"The food was delicious\"\n",
    "Sentiment: Positive\n",
    "Text: \"The dinner we had was great and the service too.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Apply the example to the model\n",
    "result = model(input_text, max_length=100)\n",
    "\n",
    "print(result[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
