# rag_workflow_mistral_api.py

"""
 RAG Workflow with YOUR Vector DB + Mistral-7B
✅ API Ready with FastAPI
✅ Multi-Query Retrieval
✅ Re-Ranking (Contextual Compression)
✅ Chatbot Memory
✅ LangSmith Tracing
"""

# --- Setup ---

from langchain.vectorstores import Chroma  # Change this if using Milvus/Weaviate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.document_compressors import CohereRerank
from langchain.retrievers import ContextualCompressionRetriever
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

import langsmith
from transformers import pipeline
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# --- Initialize LangSmith (Optional) ---
langsmith.init(project="ultimate-rag-project")

# --- 1. Load Your Existing Vector Database ---
persist_directory = "path/to/your/chroma_db"  # <-- UPDATE this to your path

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# --- 2. Load Your Local Mistral-7B Model ---
mistral_pipeline = pipeline(
    "text-generation", 
    model="path_or_model_name_to_mistral",  # <-- UPDATE this to your model checkpoint
    torch_dtype="auto", 
    device_map="auto",
    max_new_tokens=512,
    temperature=0.7
)

llm = HuggingFacePipeline(pipeline=mistral_pipeline)

# --- 3. Multi-Query Retriever ---
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=retriever,
    llm=llm
)

# --- 4. Re-Ranking ---
compressor = CohereRerank(model="bge-reranker-base")
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=multi_query_retriever
)

# --- 5. Setup Memory ---
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# --- 6. Build Conversational Retrieval Chain ---
conversational_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=compression_retriever,
    memory=memory
)

# --- 7. Trace with LangSmith (Optional) ---
conversational_chain = langsmith.traceable(conversational_chain)

# --- 8. FastAPI Setup ---

app = FastAPI()

# Enable CORS if needed
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    question: str

@app.post("/query")
async def query_rag(request: QueryRequest):
    response = conversational_chain.run({
        "question": request.question,
        "chat_history": []
    })
    return {"answer": response}

# --- 9. Run the API ---

if __name__ == "__main__":
    uvicorn.run("rag_workflow_mistral_api:app", host="0.0.0.0", port=8000, reload=True)
